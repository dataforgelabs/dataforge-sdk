{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84a9a01c-2bd0-405f-b111-9777f2f92ab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dataforge import IngestionSession\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from pyspark.sql import SparkSession, DataFrame, Column\n",
    "from pyspark.sql.functions import col, struct, lit, array\n",
    "from pyspark.sql.types import DataType, StringType, DoubleType, DateType, TimestampType, BooleanType, StructType, StructField, ArrayType\n",
    "\n",
    "session = IngestionSession()\n",
    "\n",
    "# Get custom connection parameters from DataForge connection settings\n",
    "connection_parameters = session.connection_parameters()\n",
    "access_token = connection_parameters.get(\"private_connection_parameters\")[\"access_token\"]\n",
    "\n",
    "# Get custom parameters from source settings\n",
    "custom_parameters = session.custom_parameters()\n",
    "session.log(f\"Custom Parameters: {custom_parameters}\")\n",
    "\n",
    "# Store each custom parameter in a variable for later use\n",
    "hubspot_object = custom_parameters.get(\"hubspot_object\")\n",
    "get_pipelines = custom_parameters.get(\"get_pipelines\", False)\n",
    "get_properties = custom_parameters.get(\"get_properties\", False)\n",
    "properties = custom_parameters.get(\"properties\", None)\n",
    "limit = custom_parameters.get(\"limit\", 50)\n",
    "archived = custom_parameters.get(\"archived\", None)\n",
    "properties_with_history = custom_parameters.get(\"properties_with_history\", None)\n",
    "associations = custom_parameters.get(\"associations\", None) \n",
    "include_all_properties = custom_parameters.get(\"include_all_properties\", False)\n",
    "flatten_properties = custom_parameters.get(\"flatten_properties\", False)\n",
    "flatten_properties_stage_fields = custom_parameters.get(\"flatten_properties_stage_fields\", False)\n",
    "\n",
    "#Fail the SDK session if the hubspot_object custom parameter is missing\n",
    "if (not hubspot_object):\n",
    "    session.fail(\"Missing hubspot_object. Please check custom parameters in source ingestion parameters.\")\n",
    "    raise Exception(\"Missing hubspot_object. Please check custom parameters in source ingestion parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "223f4385-a51e-402f-8624-2c2b3d489b77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to get data from HubSpot using the REST API\n",
    "def get_data(url: str, access_token: str, params: dict) -> list[dict]:    \n",
    "\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {access_token}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    all_data = []\n",
    "    after = None\n",
    "    \n",
    "    while True:\n",
    "        if after:\n",
    "            params['after'] = after\n",
    "        print(url)\n",
    "        response = requests.get(url,headers=headers,params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            all_data.extend(data['results'])\n",
    "            \n",
    "            # Check for more data\n",
    "            if 'paging' in data and 'next' in data['paging']:\n",
    "                after = data['paging']['next']['after']\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            raise Exception(f\"Error {response.status_code}: {response.json().get('message')}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "# Function to extract keys and their types from records while preserving order\n",
    "def extract_object_fields(item: dict) -> tuple[dict[str,str], dict[str,str]]:\n",
    "    root_fields = {}\n",
    "    properties_fields = {}\n",
    "\n",
    "    # Extract fields from the root level of the item\n",
    "    for key in item.keys():  # Maintain order by iterating over keys directly\n",
    "        value = item[key]  # Get the value for the key\n",
    "        if key != \"properties\":  # Skip properties for now\n",
    "            root_fields[key] = type(value).__name__  # Get the type of the value\n",
    "    \n",
    "    # Now extract fields from the properties struct\n",
    "    for key, value in item['properties'].items():\n",
    "        if key not in properties_fields:\n",
    "            properties_fields[key] = type(value).__name__  # Get the type of the property value\n",
    "\n",
    "    return root_fields, properties_fields\n",
    "\n",
    "# Function to get the schema for the properties field\n",
    "def get_object_properties_schema(object_type: str) -> tuple[list[dict], dict[str,str]]:\n",
    "    properties_results = get_data(f\"https://api.hubapi.com/crm/v3/properties/{object_type}\", access_token, params=None)\n",
    "    schema_map = {}\n",
    "\n",
    "    # Ensure we handle the structure of properties_schema correctly\n",
    "    for prop in properties_results:  # Access the results attribute\n",
    "        schema_map[prop['name']] = prop['type']  # Map property name to its type\n",
    "\n",
    "    return properties_results, schema_map\n",
    "\n",
    "# Function to map HubSpot types to Spark types\n",
    "def map_hubspot_to_spark(hubspot_type: str) -> DataType:\n",
    "    mapping = {\n",
    "        \"string\": StringType(),\n",
    "        \"number\": DoubleType(),\n",
    "        \"date\": DateType(),\n",
    "        \"datetime\": TimestampType(),\n",
    "        \"boolean\": BooleanType(),\n",
    "        \"enumeration\": StringType(),  # Assuming enum types are treated as strings\n",
    "        \"json\": StringType(),  # Assuming JSON data is treated as strings\n",
    "        # Add more mappings as necessary\n",
    "    }\n",
    "    return mapping.get(hubspot_type, StringType())  # Default to StringType\n",
    "\n",
    "# Function to create the spark schema from the list of root fields and properties struct fields, mapping them to spark types and creating the schema\n",
    "def create_spark_schema(sample_item: dict, object_schema: dict[str,str]) -> StructType:\n",
    "    #Getting list of root field names and properties struct field names\n",
    "    root_fields_order, properties_fields = extract_object_fields(sample_item)\n",
    "\n",
    "    # Create Spark StructType for properties struct field\n",
    "    properties_struct_fields = []\n",
    "    for field in properties_fields.keys():\n",
    "        spark_type = map_hubspot_to_spark(object_schema.get(field, \"string\"))  # Map HubSpot type to Spark type\n",
    "        properties_struct_fields.append(StructField(field, spark_type))\n",
    "    properties_struct_type = StructType(properties_struct_fields)\n",
    "    \n",
    "    # Creating root fields as struct fields\n",
    "    struct_fields = []\n",
    "    added_keys = set()  # Track added keys to prevent duplicates\n",
    "\n",
    "    for field, hubspot_type in root_fields_order.items(): # Only applies to non properties fields\n",
    "        if field not in added_keys:\n",
    "            # if field == 'properties':\n",
    "                \n",
    "            # else:\n",
    "                spark_type = map_hubspot_to_spark(hubspot_type)  # Map to Spark type\n",
    "                struct_fields.append(StructField(field, spark_type))\n",
    "                added_keys.add(field)  # Mark this field as added\n",
    "    struct_fields.append(StructField(\"properties\", properties_struct_type)) # Appending properties struct into schema after root fields\n",
    "\n",
    "    final_schema = StructType(struct_fields)\n",
    "    return final_schema\n",
    "\n",
    "# Function to cast dataframe columns based on spark schema recursively\n",
    "def cast_column(col_name: str, schema_field: StructField) -> Column:\n",
    "    data_type = schema_field.dataType\n",
    "    if isinstance(data_type, StructType):\n",
    "        # For nested fields, recursively cast each subfield\n",
    "        return struct([\n",
    "            cast_column(f\"{col_name}.{sub_field.name}\", sub_field).alias(sub_field.name)\n",
    "            for sub_field in data_type.fields\n",
    "        ]).alias(col_name)\n",
    "    elif isinstance(data_type, ArrayType):\n",
    "        # For ArrayType, cast the element type\n",
    "        element_type = data_type.elementType\n",
    "        return col(col_name).cast(ArrayType(element_type)).alias(col_name)\n",
    "    else:\n",
    "        # For simple types, cast directly\n",
    "        return col(col_name).cast(data_type).alias(col_name)\n",
    "    \n",
    "# Function to identify base field names that are duplicated with an ID at the end\n",
    "def identify_stage_field_names(df: DataFrame, struct_field: str) -> list[str]:\n",
    "    # Define field name pattern with ID of more than one digit at the end\n",
    "    pattern = re.compile(r\"(.+)_\\d{2,}$\")\n",
    "\n",
    "    # For storing base field names that match the pattern\n",
    "    base_field_counts = {}\n",
    "\n",
    "    # Loop through the fields of the struct\n",
    "    for field in df.schema[struct_field].dataType.fields:\n",
    "        match = pattern.match(field.name)\n",
    "        if match:\n",
    "            base_name = match.group(1)  # Extract base name (e.g. 'hs_date_entered')\n",
    "            \n",
    "            # Count occurrences of the base name\n",
    "            if base_name in base_field_counts:\n",
    "                base_field_counts[base_name] += 1\n",
    "            else:\n",
    "                base_field_counts[base_name] = 1\n",
    "\n",
    "    # Return the list of base names that have more than one occurrence of the pattern\n",
    "    base_field_names = [base_name for base_name, count in base_field_counts.items() if count > 1]\n",
    "\n",
    "    return base_field_names\n",
    "\n",
    "# Function to convert fields into an array of structs where the key is the number at the end\n",
    "def convert_fields_to_array_of_structs(df: DataFrame, struct_field: str, base_field_names: list[str]) -> DataFrame:\n",
    "    # Define field name pattern with ID of more than one digit at the end\n",
    "    valid_field_pattern = re.compile(r\"(.+)_\\d{2,}$\")\n",
    "    \n",
    "    # For each base field name, create an array of structs with key-value pairs\n",
    "    for base_field_name in base_field_names:\n",
    "        key_value_structs = []\n",
    "        fields_to_drop = []\n",
    "\n",
    "        # Get all fields from the struct that match the base field name pattern (e.g. 'hs_date_exited_*')\n",
    "        for field in df.schema[struct_field].dataType.fields:\n",
    "            # Loop through fields and to get list of fields that start with the base field name\n",
    "            if field.name.startswith(base_field_name):\n",
    "                # Check if the field ends with a valid digit ending\n",
    "                match = valid_field_pattern.match(field.name)\n",
    "                \n",
    "                if match:\n",
    "                    # Extract the numeric suffix (after the last underscore)\n",
    "                    key = field.name.split(\"_\")[-1]  # Extract '235832088' from 'hs_date_exited_235832088'\n",
    "\n",
    "                    # Create a struct with key-value pair, where key is the number and value is the field value\n",
    "                    key_value_struct = struct(lit(key).alias(\"pipeline_stage_id\"), col(f\"{struct_field}.{field.name}\").alias(\"value\"))\n",
    "\n",
    "                    # Append the struct to the list\n",
    "                    key_value_structs.append(key_value_struct)\n",
    "\n",
    "                    # Add the field name to the list of fields to drop later\n",
    "                    fields_to_drop.append(field.name)\n",
    "\n",
    "        # Combine all key-value pairs into an array of structs and create a new column for each base field name. Remove the separated fields from the properties column.\n",
    "        if key_value_structs:\n",
    "            df = df.withColumn(f\"array_{base_field_name}\", array(*key_value_structs))\n",
    "\n",
    "            # Dropping fields that were separated into columns from the original properties struct\n",
    "            df = df.withColumn(struct_field, col(struct_field).dropFields(*fields_to_drop))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to check the dataframe for repeated pipeline stage fields, convert them to columns, and return the final dataframe\n",
    "def map_dataframe(df: DataFrame, struct_field: str) -> DataFrame:\n",
    "    # Check for any pipeline stage fields in the api resulting dataframe\n",
    "    if struct_field in df.columns:\n",
    "        pipeline_stage_fields = identify_stage_field_names(df, struct_field)\n",
    "    else:\n",
    "        pipeline_stage_fields = None\n",
    "\n",
    "    # If pipeline stage fields exist, create new columns as array[struct{}] and map the data in to the new columns\n",
    "    if pipeline_stage_fields:\n",
    "        final_df = convert_fields_to_array_of_structs(df, struct_field, pipeline_stage_fields)\n",
    "    else:\n",
    "        final_df = df\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Helper function to process flattening of dataframe\n",
    "def process_dataframe(df: DataFrame, flatten_props: bool, flatten_stage_fields: bool) -> DataFrame:\n",
    "    if flatten_properties:\n",
    "        print(\"Run with flatten_properties.\")\n",
    "        # Separate pipeline stage fields into separate columns as array<struct<>> first\n",
    "        mapped_df = map_dataframe(df, \"properties\")\n",
    "\n",
    "        # Get list of all fields from mapped dataframe for flattening\n",
    "        mapped_df_fields = mapped_df.schema.names\n",
    "\n",
    "        # Flatten remaining properties fields into final dataframe\n",
    "        final_df = mapped_df.selectExpr(*mapped_df.schema.names, \"properties.*\").drop(\"properties\")\n",
    "            \n",
    "    # Determine if we should flatten pipeline stage fields into separate columns as ARRAY<STRUCT<>> so they're easier to work with\n",
    "    elif flatten_properties_stage_fields:\n",
    "        print(\"Run with flatten_properties_stage_fields.\")\n",
    "        # Separate pipeline stage fields into separate columns as array<struct<>> for final dataframe\n",
    "        final_df = map_dataframe(df, \"properties\")\n",
    "\n",
    "    # If neither flatten_properties or flatten_properties_stage_fields are true, return the dataframe as is\n",
    "    else:\n",
    "        print(\"Run with no flattening.\")\n",
    "        final_df = df\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f58ab96a-895b-4e81-9076-bbedd0554bdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get all properties of object and the schema map (used for include_all_properties and get_properties)\n",
    "try:\n",
    "    all_properties_results, properties_schema = get_object_properties_schema(hubspot_object)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Define which properties to include if any\n",
    "try:\n",
    "    if include_all_properties:\n",
    "        properties = [prop['name'] for prop in all_properties_results]\n",
    "except:\n",
    "    print(f\"properties don't exist for {hubspot_object}\")\n",
    "    pass\n",
    "\n",
    "# Parameter builder\n",
    "parameters = {\n",
    "    \"limit\": limit,\n",
    "    \"properties\": properties,\n",
    "    \"propertiesWithHistory\": properties_with_history,\n",
    "    \"associations\": associations,\n",
    "    \"archived\": archived\n",
    "}\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15827718-0820-4cb9-b714-d1db584077c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define an empty schema to be returned if the object results are empty\n",
    "schema = StructType([StructField(\"id\", StringType(), True)])\n",
    "empty_df = spark.createDataFrame([], schema)\n",
    "\n",
    "#Final function to get and return dataframe to DataForge SDK\n",
    "def get_final_dataframe():\n",
    "    \"\"\"\n",
    "    Refactored version that eliminates duplication and improves error handling\n",
    "    \"\"\"\n",
    "    def create_dataframe_safely(data, use_schema=False):\n",
    "        \"\"\"Helper function to safely create DataFrames with consistent error handling\"\"\"\n",
    "        if not data:\n",
    "            session.log(\"Empty data. Returning empty dataframe.\")\n",
    "            return empty_df\n",
    "        \n",
    "        try:\n",
    "            if use_schema and isinstance(data, list):\n",
    "                # For complex objects that need schema casting\n",
    "                df = spark.createDataFrame(data)\n",
    "                try:\n",
    "                    final_schema = create_spark_schema(data[0], properties_schema)\n",
    "                    cast_df = df.select([cast_column(field.name, field) for field in final_schema.fields])\n",
    "                    return process_dataframe(cast_df, flatten_props=flatten_properties, \n",
    "                                           flatten_stage_fields=flatten_properties_stage_fields)\n",
    "                except:\n",
    "                    # Return dataframe as is if casting fails\n",
    "                    return process_dataframe(df, flatten_props=flatten_properties, \n",
    "                                           flatten_stage_fields=flatten_properties_stage_fields)\n",
    "            else:\n",
    "                # For simple data structures\n",
    "                if isinstance(data, list) and data:\n",
    "                    # Check if we need to convert to JSON RDD first\n",
    "                    if isinstance(data[0], dict) and any(isinstance(v, (dict, list)) for v in data[0].values()):\n",
    "                        rd = spark.sparkContext.parallelize(data).map(lambda x: json.dumps(x))\n",
    "                        return spark.read.json(rd)\n",
    "                    else:\n",
    "                        return spark.createDataFrame(data)\n",
    "                else:\n",
    "                    return spark.createDataFrame(data)\n",
    "        except Exception as e:\n",
    "            session.log(f\"Exception creating DataFrame: {e}\")\n",
    "            return empty_df\n",
    "\n",
    "    # Determine URL and data source based on conditions\n",
    "    try:\n",
    "        if get_properties:\n",
    "            # Return dataframe of object properties\n",
    "            final_df = create_dataframe_safely(all_properties_results)\n",
    "            \n",
    "        elif get_pipelines:\n",
    "            # Return dataframe of object pipelines\n",
    "            pipeline_url = f\"https://api.hubapi.com/crm/v3/pipelines/{hubspot_object}\"\n",
    "            object_results = get_data(pipeline_url, access_token, params=None)\n",
    "            final_df = create_dataframe_safely(object_results)\n",
    "            \n",
    "        elif hubspot_object == \"owners\":\n",
    "            # Use different endpoint for owners\n",
    "            owners_url = f\"https://api.hubapi.com/crm/v3/{hubspot_object}\"\n",
    "            object_results = get_data(owners_url, access_token, params=None)\n",
    "            final_df = create_dataframe_safely(object_results)\n",
    "            \n",
    "        else:\n",
    "            # Standard object endpoint with schema processing\n",
    "            url = f\"https://api.hubapi.com/crm/v3/objects/{hubspot_object}\"\n",
    "            object_results = get_data(url, access_token, parameters)\n",
    "            final_df = create_dataframe_safely(object_results, use_schema=True)\n",
    "            \n",
    "    except Exception as e:\n",
    "        session.log(f\"Exception in get_final_dataframe: {e}\")\n",
    "        session.log(\"Returning empty dataframe\")\n",
    "        final_df = empty_df\n",
    "    \n",
    "    # Check if final dataframe is empty\n",
    "    if final_df.rdd.isEmpty():\n",
    "        session.log(\"No data in final results\", \"E\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "session.ingest(get_final_dataframe)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "HubSpot REST API Integration",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
